{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from cProfile import label\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.dates as mdates\n",
    "import lightgbm as lgb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline creazione train e test per ogni dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# suddivisione train-test: divido per mesi e considero le istanze \n",
    "# questa la divisione che ho usato io \n",
    "def split_train_test_by_month_istanza(df, target):\n",
    "    X_train_list = []\n",
    "    X_test_list = []\n",
    "    y_train_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    # Creo una colonna 'year_month' per identificare il mese e l'anno così da non avere problemi con gli stessi mesi di diversi anni\n",
    "    df['year_month'] = df['year'].astype(str) + '-' + df['month'].astype(str)\n",
    "    df['day_hour'] = df['day'].astype(str) + '-' + df['hour'].astype(str)\n",
    "\n",
    "    for year_month in df['year_month'].unique():\n",
    "        month_data = df[df['year_month'] == year_month]\n",
    "        hours = month_data['day_hour'].unique()\n",
    "        if len(hours) <= 1:\n",
    "            continue\n",
    "        random_seed = hash(year_month) % (2 ** 32 - 1)\n",
    "        train_hours, test_hours = train_test_split(hours, test_size=0.25, random_state=random_seed)\n",
    "\n",
    "        train_data = month_data[month_data['day_hour'].isin(train_hours)]\n",
    "        test_data = month_data[month_data['day_hour'].isin(test_hours)]\n",
    "\n",
    "        X_train_list.append(train_data.drop(columns=[target, 'year_month', 'day_hour']))\n",
    "        X_test_list.append(test_data.drop(columns=[target, 'year_month', 'day_hour']))\n",
    "        y_train_list.append(train_data[target])\n",
    "        y_test_list.append(test_data[target])\n",
    "\n",
    "    X_train = pd.concat(X_train_list)\n",
    "    X_test = pd.concat(X_test_list)\n",
    "    y_train = pd.concat(y_train_list)\n",
    "    y_test = pd.concat(y_test_list)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Pipeline completa\n",
    "def pipeline_train_test(df):\n",
    "    target = 'pm2p5_y'\n",
    "    city = df['city'].iloc[0]\n",
    "    type_sensor = df['type_sensor'].iloc[0]\n",
    "    if type_sensor == 'PMS7003':\n",
    "        type_sensor = 'AIRBEAM'\n",
    "    X_train, X_test, y_train, y_test = split_train_test_by_month_istanza(df, target)\n",
    "\n",
    "    training = pd.concat([X_train, y_train], axis=1)\n",
    "    testing = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    if city == 'Southampton' or city == 'Lima':\n",
    "        #concatena nome città e sensore \n",
    "        city = city + '_' + type_sensor\n",
    "\n",
    "    #salvo le divisioni da poter usare successivamente\n",
    "    training.to_csv(f'../dataset/training_{city}.csv', index=False)\n",
    "    testing.to_csv(f'../dataset/testing_{city}.csv', index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# aosta = pd.read_csv('../dataset/aosta.csv')\n",
    "# pipeline_train_test(aosta)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST A COPPIE E PROGRESSIVO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# elimino le features non importanti: city(dato che uso lat e lon) e valid_at\n",
    "def delete_features(X_train, X_test, column):\n",
    "    # Assicurati che datetime_column sia una lista\n",
    "    if isinstance(column, str):\n",
    "        column = [column]\n",
    "    for col in column:\n",
    "        X_train = X_train.drop(columns=col)\n",
    "        X_test = X_test.drop(columns=col)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "#funzione per gestire i valori mancanti --> non ce ne sono \n",
    "\"\"\" def handle_missing_values(X_train, X_test, strategy='drop'):\n",
    "    if strategy == 'drop':\n",
    "        return X_train.dropna(), X_test.dropna()\n",
    "    elif strategy == 'mean':\n",
    "        return X_train.fillna(X_train.mean()), X_test.fillna(X_test.mean())\n",
    "    else:\n",
    "        raise ValueError(\"Strategia non supportata. Usa 'drop' o 'mean'.\") \"\"\"\n",
    "\n",
    "\n",
    "#funzione per preprocessare le variabili categoriche: type_sensor, sensor_id, day_of_week\n",
    "def preprocess_categorical(X, categorical_columns):\n",
    "    for col in categorical_columns:\n",
    "        if col == 'sensor_id' or col == 'type_sensor':  #rendo numerici\n",
    "            X[col] = pd.Categorical(X[col])\n",
    "            X[col] = X[col].cat.codes\n",
    "        else:\n",
    "            X = pd.get_dummies(X, columns=[col], drop_first=True)  #day_of_week faccio il one hot encoding\n",
    "    return X\n",
    "\n",
    "\n",
    "#funzione per trasformare in variabili cicliche le variabili temporali\n",
    "def add_cyclic_features(X_train, X_test, columns):\n",
    "    for col in columns:\n",
    "        X_train[col + '_sin'] = np.sin(2 * np.pi * X_train[col] / X_train[col].max())\n",
    "        X_train[col + '_cos'] = np.cos(2 * np.pi * X_train[col] / X_train[col].max())\n",
    "        X_test[col + '_sin'] = np.sin(2 * np.pi * X_test[col] / X_test[col].max())\n",
    "        X_test[col + '_cos'] = np.cos(2 * np.pi * X_test[col] / X_test[col].max())\n",
    "        X_train = X_train.drop(columns=[col])\n",
    "        X_test = X_test.drop(columns=[col])\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "#definizione pipeline preprocessamento\n",
    "def preprocess_pipeline_simple_model(X_train, X_test, column, categorical_columns, strategy):\n",
    "    #X_train, X_test = delete_features(X_train, X_test, column) non elimino subito la colonna perchè uso valid_at per il plot\n",
    "    #X_train, X_test = handle_missing_values(X_train, X_test, strategy)\n",
    "    X_train = preprocess_categorical(X_train, categorical_columns)\n",
    "    X_test = preprocess_categorical(X_test, categorical_columns)\n",
    "    X_train, X_test = add_cyclic_features(X_train, X_test, ['hour', 'day', 'month', 'year'])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "#funzione per scalare le feature\n",
    "def scale_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "# Valutazione del modello\n",
    "def evaluate_model(model, X_test, y_test, y_pred):\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "\n",
    "#funzione per addestrare e valutare il modello\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = evaluate_model(model, X_test, y_test, y_pred)\n",
    "    return metrics, model, y_pred\n",
    "\n",
    "\n",
    "def plot_predictions(y_pred, X_test_plot):\n",
    "    tempo = X_test_plot['valid_at']\n",
    "    pm_orig = X_test_plot['pm2p5_x']\n",
    "    pm_pred = y_pred\n",
    "    pm_ref = X_test_plot['pm2p5_y']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.grid(True)\n",
    "    ax.plot(tempo, pm_orig, label='PM2.5 originale', color='red')\n",
    "    ax.plot(tempo, pm_pred, label='PM2.5 predetto', color='blue')\n",
    "    ax.plot(tempo, pm_ref, label='PM2.5 ref station', color='black')\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('PM2.5')\n",
    "    plt.title('PM2.5 originale vs PM2.5 predetto')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def full_pipeline(X_train, y_train, X_test, y_test, model, column, categorical_columns, strategy):\n",
    "    X_train, X_test = preprocess_pipeline_simple_model(X_train, X_test, column, categorical_columns, strategy)\n",
    "    assert X_train.shape[1] == X_test.shape[1], \"Mismatch in feature dimensions after preprocessing!\"\n",
    "\n",
    "    #faccio la copia di X_test per il plot\n",
    "    X_test_plot = X_test.copy()\n",
    "    X_test_plot['pm2p5_y'] = y_test\n",
    "    X_train, X_test = delete_features(X_train, X_test, column)\n",
    "\n",
    "    #matrice correlazione\n",
    "    \"\"\" corr = X_train.corr().round(2)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "    plt.show()  \"\"\"\n",
    "\n",
    "    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "    metrics, model, y_pred = train_and_evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    plot_predictions(y_pred, X_test_plot)\n",
    "    print(metrics)\n",
    "    return metrics, model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST PROGRESSIVO: DATASET TARGET --> TRAIN = TRAINING DATASET TARGET + ALTRO DATASET AGGIUNTO UNO ALLA VOLTA E TEST = TESTING DATASET TARGET"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#carico i singoli dataset \n",
    "aosta = pd.read_csv('../dataset/aosta.csv')\n",
    "badajoz = pd.read_csv('../dataset/badajoz.csv')\n",
    "bangalore = pd.read_csv('../dataset/bangalore.csv')\n",
    "calgary = pd.read_csv('../dataset/calgary.csv')\n",
    "delhi = pd.read_csv('../dataset/delhi.csv')\n",
    "hamirpur = pd.read_csv('../dataset/hamirpur.csv')\n",
    "lima_iqair = pd.read_csv('../dataset/lima_iqair.csv')\n",
    "lima_airbeam = pd.read_csv('../dataset/lima_airbeam.csv')\n",
    "uk_pms5003 = pd.read_csv('../dataset/uk_pms5003.csv')\n",
    "uk_sps030 = pd.read_csv('../dataset/uk_sps030.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_train_test(aosta)\n",
    "pipeline_train_test(badajoz)\n",
    "pipeline_train_test(bangalore)\n",
    "pipeline_train_test(calgary)\n",
    "pipeline_train_test(delhi)\n",
    "pipeline_train_test(hamirpur)\n",
    "pipeline_train_test(lima_iqair)\n",
    "pipeline_train_test(lima_airbeam)\n",
    "pipeline_train_test(uk_pms5003)\n",
    "pipeline_train_test(uk_sps030)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "metrics_incremental_df = pd.DataFrame(columns=['city', 'type_sensor', 'num dataset', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "#definizione dei dataset caricati in precedenza\n",
    "datasets = [aosta, badajoz, bangalore, calgary, delhi, hamirpur, lima_iqair, lima_airbeam, uk_pms5003, uk_sps030]\n",
    "\n",
    "#cicla su tutti i dataset\n",
    "for dataset in datasets:\n",
    "    city = dataset['city'].iloc[0]\n",
    "    print(f\"Testo la città {city}\")\n",
    "    type_sensor = dataset['type_sensor'].iloc[0]\n",
    "    if type_sensor == 'PMS7003' and city == 'Lima':\n",
    "        type_sensor = 'AIRBEAM'\n",
    "    if city == 'Southampton' or city == 'Lima':\n",
    "        #concatena nome città e sensore \n",
    "        city = city + '_' + type_sensor\n",
    "\n",
    "    training = pd.read_csv(f'../dataset/training_{city}.csv', parse_dates=['valid_at'])\n",
    "    testing = pd.read_csv(f'../dataset/testing_{city}.csv', parse_dates=['valid_at'])\n",
    "\n",
    "    num_datasets = 0\n",
    "\n",
    "    #itero sui dataset\n",
    "    for data in datasets:\n",
    "        num_datasets += 1\n",
    "        city_2 = data['city'].iloc[0]\n",
    "        type_sensor_2 = data['type_sensor'].iloc[0]\n",
    "        if city == city_2 and type_sensor == type_sensor_2 or city == city_2:\n",
    "            continue\n",
    "        else:\n",
    "            training = pd.concat([training, data], axis=0)\n",
    "            var = training['city'].unique()\n",
    "            var = ', '.join(var)\n",
    "            print('Traino sui dataset con le città aggiunte', training['city'].unique())\n",
    "            X_train = training.drop(columns=['pm2p5_y', 'year_month', 'day_hour'])\n",
    "            y_train = training['pm2p5_y']\n",
    "            X_test = testing.drop(columns='pm2p5_y')\n",
    "            y_test = testing['pm2p5_y']\n",
    "            model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500,\n",
    "                                      random_state=982, verbose=0)\n",
    "            metrics, model = full_pipeline(X_train, y_train, X_test, y_test, model, ['city', 'valid_at'],\n",
    "                                           categorical_columns, 'drop')\n",
    "            new_row = {'city': var, 'type_sensor': type_sensor, 'num dataset': num_datasets, 'R2': metrics['R2']}\n",
    "            metrics_incremental_df = pd.concat([metrics_incremental_df, pd.DataFrame(new_row, index=[0])],\n",
    "                                               ignore_index=True)\n",
    "\n",
    "metrics_incremental_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_incremental_df.to_csv('../dataset/metrics_incremental_df.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST A COPPIE: DATASET TARGET --> TRAIN = TRAINING DATASET TARGET + ALTRO DATASET E TEST = TESTING DATASET TARGET"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "metrics_df_coppie = pd.DataFrame(columns=['city', 'sensor', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "datasets = [aosta, badajoz, bangalore, calgary, delhi, hamirpur, lima_iqair, lima_airbeam, uk_pms5003, uk_sps030]\n",
    "#cicla su tutti i dataset\n",
    "for dataset in datasets:\n",
    "    city = dataset['city'].iloc[0]\n",
    "    type_sensor = dataset['type_sensor'].iloc[0]\n",
    "    if type_sensor == 'PMS7003' and city == 'Lima':\n",
    "        type_sensor = 'AIRBEAM'\n",
    "    if city == 'Southampton' or city == 'Lima':\n",
    "        #concatena nome città e sensore \n",
    "        city = city + '_' + type_sensor\n",
    "\n",
    "    training = pd.read_csv(f'../dataset/training_{city}.csv', parse_dates=['valid_at'])\n",
    "    testing = pd.read_csv(f'../dataset/testing_{city}.csv', parse_dates=['valid_at'])\n",
    "\n",
    "    #itero sui dataset\n",
    "    for data in datasets:\n",
    "        city_2 = data['city'].iloc[0]\n",
    "        type_sensor_2 = data['type_sensor'].iloc[0]\n",
    "        if city == city_2 and type_sensor == type_sensor_2 or city == city_2:\n",
    "            continue\n",
    "        else:\n",
    "            training_new = pd.concat([training, data], axis=0,\n",
    "                                     ignore_index=True)  #aggiungo ciclamente un dataset al training\n",
    "            var = training_new['city'].unique()\n",
    "            var = ', '.join(var)\n",
    "            print('Traino sui dataset con le città:', training_new['city'].unique())\n",
    "            #stampa cittò e sensori \n",
    "            print('Traino sui dataset con i sensori:', training_new['type_sensor'].unique())\n",
    "            #stampa i valori di pm2p5_y\n",
    "            X_train = training_new.drop(columns=['pm2p5_y', 'day_hour', 'year_month'])\n",
    "            y_train = training_new['pm2p5_y']\n",
    "            X_test = testing.drop(columns='pm2p5_y')\n",
    "            y_test = testing['pm2p5_y']\n",
    "            model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500,\n",
    "                                      random_state=982, verbose=0)\n",
    "            metrics, model = full_pipeline(X_train, y_train, X_test, y_test, model, ['city', 'valid_at'],\n",
    "                                           categorical_columns, 'drop')\n",
    "            new_row = {'city': var, 'sensor': type_sensor_2, 'R2': metrics['R2']}\n",
    "            metrics_df_coppie = pd.concat([metrics_df_coppie, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_coppie"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_coppie.to_csv('../dataset/metrics_df_coppie.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Generalizzazione\n",
    "# 1. intero dataset come test\n",
    "# 2. 25% dataset target e training restante 75% dataset target + altri dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# aggiunta di feature temporali\n",
    "def delete_features(X_train, X_test, column):\n",
    "    # Assicurati che datetime_column sia una lista\n",
    "    if isinstance(column, str):\n",
    "        column = [column]\n",
    "    for col in column:\n",
    "        X_train = X_train.drop(columns=col)\n",
    "        X_test = X_test.drop(columns=col)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "#funzione per gestire i valori mancanti\n",
    "def handle_missing_values(X_train, X_test, strategy='drop'):\n",
    "    if strategy == 'drop':\n",
    "        return X_train.dropna(), X_test.dropna()\n",
    "    elif strategy == 'mean':\n",
    "        return X_train.fillna(X_train.mean()), X_test.fillna(X_test.mean())\n",
    "    else:\n",
    "        raise ValueError(\"Strategia non supportata. Usa 'drop' o 'mean'.\")\n",
    "\n",
    "\n",
    "#funzione per preprocessare le variabili categoriche\n",
    "def preprocess_categorical(X, categorical_columns):\n",
    "    for col in categorical_columns:\n",
    "        if col == 'sensor_id' or col == 'type_sensor':  #gestione con lat e long\n",
    "            X[col] = pd.Categorical(X[col])\n",
    "            X[col] = X[col].cat.codes\n",
    "        else:\n",
    "            X = pd.get_dummies(X, columns=[col], drop_first=True)\n",
    "    return X\n",
    "\n",
    "\n",
    "#funzione per trasformare in variabili cicliche\n",
    "def add_cyclic_features(X_train, X_test, columns):\n",
    "    for col in columns:\n",
    "        X_train[col + '_sin'] = np.sin(2 * np.pi * X_train[col] / X_train[col].max())\n",
    "        X_train[col + '_cos'] = np.cos(2 * np.pi * X_train[col] / X_train[col].max())\n",
    "        X_test[col + '_sin'] = np.sin(2 * np.pi * X_test[col] / X_test[col].max())\n",
    "        X_test[col + '_cos'] = np.cos(2 * np.pi * X_test[col] / X_test[col].max())\n",
    "        X_train = X_train.drop(columns=[col])\n",
    "        X_test = X_test.drop(columns=[col])\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "#definizione pipeline preprocessamento\n",
    "def preprocess_pipeline_simple_model(X_train, X_test, column, categorical_columns, strategy):\n",
    "    #X_train, X_test = delete_features(X_train, X_test, column)\n",
    "    X_train, X_test = handle_missing_values(X_train, X_test, strategy)\n",
    "    X_train = preprocess_categorical(X_train, categorical_columns)\n",
    "    X_test = preprocess_categorical(X_test, categorical_columns)\n",
    "    X_train, X_test = add_cyclic_features(X_train, X_test, ['hour', 'day', 'month', 'year'])\n",
    "\n",
    "    return X_train, X_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "#funzione per scalare le feature\n",
    "def scale_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "# Valutazione del modello\n",
    "def evaluate_model(model, X_test, y_test, y_pred=None):\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "\n",
    "#funzione per addestrare e valutare il modello\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = evaluate_model(model, X_test, y_test, y_pred)\n",
    "    return metrics, model, y_pred\n",
    "\n",
    "\n",
    "def plot_predictions(y_pred, X_test_plot):\n",
    "    tempo = X_test_plot['valid_at']\n",
    "    pm_orig = X_test_plot['pm2p5_x']\n",
    "    pm_pred = y_pred\n",
    "    pm_ref = X_test_plot['pm2p5_y']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.grid(True)\n",
    "    ax.plot(tempo, pm_orig, label='PM2.5 originale', color='red')\n",
    "    ax.plot(tempo, pm_pred, label='PM2.5 predetto', color='blue')\n",
    "    ax.plot(tempo, pm_ref, label='PM2.5 ref station', color='black')\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    #ruota etichette data\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('PM2.5')\n",
    "    plt.title('PM2.5 originale vs PM2.5 predetto')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, column, categorical_columns, strategy):\n",
    "    X_train, X_test = preprocess_pipeline_simple_model(X_train, X_test, column, categorical_columns, strategy)\n",
    "    assert X_train.shape[1] == X_test.shape[1], \"Mismatch in feature dimensions after preprocessing!\"\n",
    "    X_test_plot = X_test.copy()\n",
    "    X_test_plot['pm2p5_y'] = y_test\n",
    "    X_train, X_test = delete_features(X_train, X_test, column)\n",
    "    #matrice correlazione\n",
    "    \"\"\" corr = X_train.corr().round(2)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "    plt.show() \"\"\"\n",
    "\n",
    "    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "    metrics, model, y_pred = train_and_evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    plot_predictions(y_pred, X_test_plot)\n",
    "    print(metrics)\n",
    "    return metrics, model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intero dataset come test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df = pd.read_csv('../dataset/all_dataset.csv', parse_dates=['valid_at'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#dataframe per metriche\n",
    "metrics_df_intero = pd.DataFrame(columns=['city', 'type_sensor', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "# Itera sulle città\n",
    "for city in df['city'].unique():\n",
    "    i = 0\n",
    "    # Filtra i sensori che sono presenti solo in quella città\n",
    "    city_data = df[df['city'] == city]\n",
    "\n",
    "    # Itera sui sensori presenti per quella città\n",
    "    for sensor in city_data['type_sensor'].unique():\n",
    "        #il mio test diventano le righe con quella città e sensore, il target è pm2p5_y\n",
    "        X_test = df[(df['city'] == city) & (df['type_sensor'] == sensor)].drop(columns=['pm2p5_y'])\n",
    "        y_test = df[(df['city'] == city) & (df['type_sensor'] == sensor)]['pm2p5_y']\n",
    "        X_train = df[(df['city'] != city) & (df['type_sensor'] != sensor)].drop(columns=['pm2p5_y'])\n",
    "        y_train = df[(df['city'] != city) & (df['type_sensor'] != sensor)]['pm2p5_y']\n",
    "\n",
    "        print(f\"Testo la città {city} e il sensore {sensor}\")\n",
    "        model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                                  verbose=0)\n",
    "        metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                          categorical_columns, 'drop')\n",
    "        #aggiungi le metriche al dataframe\n",
    "        new_row = {'city': city, 'type_sensor': sensor, 'MAE': metrics['MAE'], 'MSE': metrics['MSE'],\n",
    "                   'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "        metrics_df_intero = pd.concat([metrics_df_intero, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_intero"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_intero.to_csv('../dataset/metrics_df_intero.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stessa analisi precedente, ma aggiungo il training del dataset target: 25% dataset target e training restante 75% dataset target + altri dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#carico i singoli dataset \n",
    "aosta = pd.read_csv('../dataset/aosta.csv')\n",
    "badajoz = pd.read_csv('../dataset/badajoz.csv')\n",
    "bangalore = pd.read_csv('../dataset/bangalore.csv')\n",
    "calgary = pd.read_csv('../dataset/calgary.csv')\n",
    "delhi = pd.read_csv('../dataset/delhi.csv')\n",
    "hamirpur = pd.read_csv('../dataset/hamirpur.csv')\n",
    "lima_iqair = pd.read_csv('../dataset/lima_iqair.csv')\n",
    "lima_airbeam = pd.read_csv('../dataset/lima_airbeam.csv')\n",
    "uk_pms5003 = pd.read_csv('../dataset/uk_pms5003.csv')\n",
    "uk_sps030 = pd.read_csv('../dataset/uk_sps030.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#lista dei dataset caricati \n",
    "datasets = [aosta, badajoz, bangalore, calgary, delhi, hamirpur, lima_iqair, lima_airbeam, uk_pms5003, uk_sps030]\n",
    "metrics_df_new = pd.DataFrame(columns=['city', 'type_sensor', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "for data in datasets:\n",
    "    city = data['city'].iloc[0]\n",
    "    type_sensor = data['type_sensor'].iloc[0]\n",
    "    if type_sensor == 'PMS7003' and city == 'Lima':\n",
    "        type_sensor = 'AIRBEAM'\n",
    "    if city == 'Southampton' or city == 'Lima':\n",
    "        #concatena nome città e sensore \n",
    "        city = city + '_' + type_sensor\n",
    "\n",
    "    city_data = df[df['city'] == city]\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(city_data) * 0.25)\n",
    "\n",
    "    # Perform the sequential split\n",
    "    training = city_data.iloc[:split_index]\n",
    "    testing = city_data.iloc[split_index:]\n",
    "\n",
    "    training = pd.read_csv(f'../dataset/training_{city}.csv', parse_dates=['valid_at'])\n",
    "    testing = pd.read_csv(f'../dataset/testing_{city}.csv', parse_dates=['valid_at'])\n",
    "    #concatena al training tutti gli altri dataset tranne quello corrente\n",
    "    for dataset in datasets:\n",
    "        if dataset is not data:\n",
    "            training = pd.concat([training, dataset])\n",
    "\n",
    "    X_train = training.drop(columns=['pm2p5_y', 'year_month', 'day_hour'])\n",
    "    y_train = training['pm2p5_y']\n",
    "    X_test = testing.drop(columns=['pm2p5_y'])\n",
    "    y_test = testing['pm2p5_y']\n",
    "\n",
    "    model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                              verbose=0)\n",
    "    metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                      categorical_columns, 'drop')\n",
    "    #aggiungi le metriche al dataframe\n",
    "    new_row = {'city': city, 'type_sensor': type_sensor, 'MAE': metrics['MAE'], 'MSE': metrics['MSE'],\n",
    "               'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "    metrics_df_new = pd.concat([metrics_df_new, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_new"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_new.to_csv('../dataset/metrics_df_25_ele.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PCA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.decomposition import PCA\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../dataset/all_dataset.csv')\n",
    "\n",
    "#\n",
    "# # df not categorical to categorical\n",
    "# df['day_of_week'] = pd.Categorical(df['day_of_week'])\n",
    "# df['city'] = pd.Categorical(df['city'])\n",
    "# df['sensor_id'] = pd.Categorical(df['sensor_id'])\n",
    "# df['type_sensor'] = pd.Categorical(df['type_sensor'])\n",
    "#\n",
    "# cities_dic = {i: city for i, city in enumerate(df['city'].unique())}\n",
    "# print(cities_dic)\n",
    "#\n",
    "# df['day_of_week'] = pd.Categorical(df['day_of_week']).codes\n",
    "# df['city'] = pd.Categorical(df['city']).codes\n",
    "# df['sensor_id'] = pd.Categorical(df['sensor_id']).codes\n",
    "# df['type_sensor'] = pd.Categorical(df['type_sensor']).codes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['city'] = df.apply(\n",
    "    lambda x: (\n",
    "        x['city'] + '_IQAir' if x['city'] == 'Lima' and x['type_sensor'] == 'IQAir'\n",
    "        else x['city'] + '_AIRBEAM' if x['city'] == 'Lima' and x['type_sensor'] != 'IQAir'\n",
    "        else x['city'] + '_' + x['type_sensor'] if x['city'] == 'Southampton'\n",
    "        else x['city']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# concat datasets and perform pca\n",
    "# df_for_pca = df.drop(columns=['valid_at'])\n",
    "df_for_pca = df[['pm2p5_x', 'pm2p5_y', 'relative_humidity', 'temperature', 'wind_speed', 'rain']]\n",
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_res = pca_2.fit_transform(df_for_pca)\n",
    "pca_df_2 = pd.DataFrame(data=pca_res, columns=['principal_component_1', 'principal_component_2'])\n",
    "print(pca_2.explained_variance_ratio_)\n",
    "print(pca_2.singular_values_)\n",
    "# total variance\n",
    "print(pca_2.explained_variance_ratio_.sum())\n",
    "\n",
    "pca_3 = PCA(n_components=3)\n",
    "pca_res = pca_3.fit_transform(df_for_pca)\n",
    "pca_df_3 = pd.DataFrame(data=pca_res,\n",
    "                        columns=['principal_component_1', 'principal_component_2', 'principal_component_3'])\n",
    "print(pca_3.explained_variance_ratio_)\n",
    "print(pca_3.singular_values_)\n",
    "print(pca_3.explained_variance_ratio_.sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add city to pca_df\n",
    "pca_df_2['city'] = df['city']\n",
    "pca_df_3['city'] = df['city']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # for each dataset plot in red the dataset and leave the rest black\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=15)\n",
    "ax.set_title('2 component PCA', fontsize=20)\n",
    "\n",
    "targets = pca_df_2['city'].unique()\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = pca_df_2['city'] == target\n",
    "    ax.scatter(pca_df_2.loc[indicesToKeep, 'principal_component_1'],\n",
    "               pca_df_2.loc[indicesToKeep, 'principal_component_2'],\n",
    "               c=color,\n",
    "               alpha=0.5,\n",
    "               s=50,\n",
    "               label=target)\n",
    "plt.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pca visualization for 3 dimension with rotation with seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(pca_df_2, hue='city', palette='bright')\n",
    "\n",
    "# put legend in top left corner inside the plot\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(data=pca_df_2, x='principal_component_1', y='principal_component_2', hue='city', palette='bright')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pca visualization for 3 dimension with rotation with seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(pca_df_3, hue='city', palette='bright')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column in ['pm2p5_x', 'pm2p5_y', 'relative_humidity', 'temperature', 'wind_speed', 'rain']:\n",
    "    pca_df_3[column] = df[column]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # pca visualization for 3 dimension hue based on relative_humidity with ranges\n",
    "    sns.scatterplot(data=pca_df_3, x='principal_component_1', y='principal_component_2', hue=column)\n",
    "    plt.title('C1 - C2: ' + column)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # pca visualization for 3 dimension hue based on relative_humidity with ranges\n",
    "    sns.scatterplot(data=pca_df_3, x='principal_component_2', y='principal_component_3', hue=column)\n",
    "    plt.title('C2 - C3: ' + column)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # pca visualization for 3 dimension hue based on relative_humidity with ranges\n",
    "    sns.scatterplot(data=pca_df_3, x='principal_component_1', y='principal_component_3', hue=column)\n",
    "    plt.title('C1 - C3: ' + column)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # pca visualization for 3 dimension hue based on relative_humidity with ranges\n",
    "# pca_df_3['relative_humidity'] = df['relative_humidity']\n",
    "# # pca_df_3['relative_humidity'] = pd.cut(pca_df_3['relative_humidity'], bins=5)\n",
    "# sns.pairplot(pca_df_3, hue='relative_humidity')\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Principal Component 1', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=15)\n",
    "ax.set_zlabel('Principal Component 3', fontsize=15)\n",
    "ax.set_title('3 component PCA', fontsize=20)\n",
    "\n",
    "targets = pca_df_3['city'].unique()\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = pca_df_3['city'] == target\n",
    "    ax.scatter(pca_df_3.loc[indicesToKeep, 'principal_component_1'],\n",
    "               pca_df_3.loc[indicesToKeep, 'principal_component_2'],\n",
    "               pca_df_3.loc[indicesToKeep, 'principal_component_3'],\n",
    "               c=color,\n",
    "               alpha=0.5,\n",
    "               s=50,\n",
    "               label=target)\n",
    "plt.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# use pca for selecting datasets",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a function to calculate Euclidean distances\n",
    "def calculate_intersected_data(pca_df, number_of_components, target_city):\n",
    "    if number_of_components == 2:\n",
    "        target_data = pca_df[pca_df['city'] == target_city][['principal_component_1', 'principal_component_2']]\n",
    "        other_data = pca_df[pca_df['city'] != target_city][['principal_component_1', 'principal_component_2']]\n",
    "    else:\n",
    "        target_data = pca_df[pca_df['city'] == target_city][\n",
    "            ['principal_component_1', 'principal_component_2', 'principal_component_3']]\n",
    "        other_data = pca_df[pca_df['city'] != target_city][\n",
    "            ['principal_component_1', 'principal_component_2', 'principal_component_3']]\n",
    "\n",
    "    if number_of_components == 2:\n",
    "        target_pc1_min, target_pc1_max = target_data['principal_component_1'].min(axis=0), target_data[\n",
    "            'principal_component_1'].max(axis=0)\n",
    "        target_pc2_min, target_pc2_max = target_data['principal_component_2'].min(axis=0), target_data[\n",
    "            'principal_component_2'].max(axis=0)\n",
    "    else:\n",
    "        target_pc1_min, target_pc1_max = target_data['principal_component_1'].min(axis=0), target_data[\n",
    "            'principal_component_1'].max(axis=0)\n",
    "        target_pc2_min, target_pc2_max = target_data['principal_component_2'].min(axis=0), target_data[\n",
    "            'principal_component_2'].max(axis=0)\n",
    "        target_pc3_min, target_pc3_max = target_data['principal_component_3'].min(axis=0), target_data[\n",
    "            'principal_component_3'].max(axis=0)\n",
    "\n",
    "    addition = 50\n",
    "\n",
    "    # select in other_data the data that are between min and max of the different components\n",
    "    if number_of_components == 3:\n",
    "        data_intersected = other_data[\n",
    "            (other_data['principal_component_1'] >= target_pc1_min - addition) & (\n",
    "                    other_data['principal_component_1'] <= target_pc1_max + addition) &\n",
    "            (other_data['principal_component_2'] >= target_pc2_min - addition) & (\n",
    "                    other_data['principal_component_2'] <= target_pc2_max + addition) &\n",
    "            (other_data['principal_component_3'] >= target_pc3_min - addition) & (\n",
    "                    other_data['principal_component_3'] <= target_pc3_max + addition)]\n",
    "    else:\n",
    "        data_intersected = other_data[\n",
    "            (other_data['principal_component_1'] >= target_pc1_min - addition) & (\n",
    "                    other_data['principal_component_1'] <= target_pc1_max + addition) &\n",
    "            (other_data['principal_component_2'] >= target_pc2_min - addition) & (\n",
    "                    other_data['principal_component_2'] <= target_pc2_max + addition)]\n",
    "\n",
    "    return data_intersected.index\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## tutto e 75 con 2 componenti"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train a model for each city using nearby cities' data\n",
    "for target_city in pca_df_3['city'].unique():\n",
    "    data_intersected_indexes = calculate_intersected_data(pca_df_3, 2, target_city)\n",
    "\n",
    "    print(f\"Intersected data for {target_city}: {data_intersected_indexes}\")\n",
    "\n",
    "    pca_df_3_ = pca_df_3.copy()\n",
    "    print(pca_df_3_.columns)\n",
    "    # add column: where city is target_city put 1, where index is in data_intersected_indexes put 2, else 0\n",
    "    pca_df_3_['intersection'] = pca_df_3_.apply(\n",
    "        lambda x: 1 if x['city'] == target_city else 2 if x.name in data_intersected_indexes else 0, axis=1)\n",
    "\n",
    "    results[target_city] = pca_df_3_\n",
    "\n",
    "    # scatterplot pca_df_3_ based on intersection class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=pca_df_3_, x='principal_component_1', y='principal_component_2', hue='intersection')\n",
    "    plt.title(f'PCA 2D for {target_city}')\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# in results for each city print how many values for each one of the other cities are set to 2\n",
    "for city, data in results.items():\n",
    "    print(f\"City: {city}\")\n",
    "    print(data['intersection'].value_counts())\n",
    "\n",
    "    # print in intersection of value = 2 how many for each unique city\n",
    "    print(data[data['intersection'] == 2]['city'].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### tutto"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_df_pca = pd.DataFrame(columns=['city', 'intersected_cities', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "for city, data in results.items():\n",
    "    target_city_indexes = data[data['intersection'] == 1].index\n",
    "    intersected_cities_indexes = data[data['intersection'] == 2].index\n",
    "\n",
    "    target_city = df[df.index.isin(target_city_indexes)]\n",
    "    intersected_cities = df[df.index.isin(intersected_cities_indexes)]\n",
    "\n",
    "    print(f\"Testo la città {city}\")\n",
    "    # print unique cities value count\n",
    "    print(target_city['city'].value_counts())\n",
    "    print(intersected_cities['city'].value_counts())\n",
    "\n",
    "    if len(intersected_cities) == 0:\n",
    "        continue\n",
    "\n",
    "    X_train = intersected_cities.drop(columns=['pm2p5_y'])\n",
    "    y_train = intersected_cities['pm2p5_y']\n",
    "    X_test = target_city.drop(columns=['pm2p5_y'])\n",
    "    y_test = target_city['pm2p5_y']\n",
    "    model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                              verbose=0)\n",
    "    metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                      categorical_columns, 'drop')\n",
    "    #aggiungi le metriche al dataframe\n",
    "    new_row = {'city': city, 'intersected_cities': \", \".join(map(str, intersected_cities['city'].unique())),\n",
    "               'MAE': metrics['MAE'], 'MSE': metrics['MSE'], 'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "    metrics_df_pca = pd.concat([metrics_df_pca, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_pca"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_pca.to_csv('../dataset/metrics_df_pca_2_intero.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 75%"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_df_pca_75 = pd.DataFrame(columns=['city', 'intersected_cities', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "for city, data in results.items():\n",
    "    intersected_cities_indexes = data[data['intersection'] == 2].index\n",
    "\n",
    "    target_city_training = pd.read_csv(f'../dataset/training_{city}.csv', parse_dates=['valid_at'])\n",
    "    target_city_testing = pd.read_csv(f'../dataset/testing_{city}.csv', parse_dates=['valid_at'])\n",
    "\n",
    "    intersected_cities = df[df.index.isin(intersected_cities_indexes)]\n",
    "    if len(intersected_cities) == 0:\n",
    "        continue\n",
    "\n",
    "    training = pd.concat([intersected_cities, target_city_training])\n",
    "\n",
    "    print(f\"Testo la città {city}\")\n",
    "    # print unique cities value count\n",
    "    print(training['city'].value_counts())\n",
    "    print(target_city_testing['city'].value_counts())\n",
    "\n",
    "    X_train = training.drop(columns=['pm2p5_y'])\n",
    "    y_train = training['pm2p5_y']\n",
    "    X_test = target_city_testing.drop(columns=['pm2p5_y'])\n",
    "    y_test = target_city_testing['pm2p5_y']\n",
    "    model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                              verbose=0)\n",
    "    metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                      categorical_columns, 'drop')\n",
    "    #aggiungi le metriche al dataframe\n",
    "    new_row = {'city': city, 'intersected_cities': \", \".join(map(str, intersected_cities['city'].unique())),\n",
    "               'MAE': metrics['MAE'], 'MSE': metrics['MSE'], 'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "    metrics_df_pca_75 = pd.concat([metrics_df_pca_75, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_pca_75"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_pca_75.to_csv('../dataset/metrics_df_pca_2_75.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 25 initial"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_df_pca_75_init = pd.DataFrame(columns=['city', 'intersected_cities', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "for city, data in results.items():\n",
    "    intersected_cities_indexes = data[data['intersection'] == 2].index\n",
    "\n",
    "    target_city_indexes = data[data['intersection'] == 1].index\n",
    "    target_city = df[df.index.isin(target_city_indexes)]\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(target_city) * 0.25)\n",
    "\n",
    "    # Perform the sequential split\n",
    "    target_city_training = target_city.iloc[:split_index]\n",
    "    target_city_testing = target_city.iloc[split_index:]\n",
    "\n",
    "    intersected_cities = df[df.index.isin(intersected_cities_indexes)]\n",
    "    if len(intersected_cities) == 0:\n",
    "        continue\n",
    "\n",
    "    training = pd.concat([intersected_cities, target_city_training])\n",
    "\n",
    "    print(f\"Testo la città {city}\")\n",
    "    # print unique cities value count\n",
    "    print(training['city'].value_counts())\n",
    "    print(target_city_testing['city'].value_counts())\n",
    "\n",
    "    X_train = training.drop(columns=['pm2p5_y'])\n",
    "    y_train = training['pm2p5_y']\n",
    "    X_test = target_city_testing.drop(columns=['pm2p5_y'])\n",
    "    y_test = target_city_testing['pm2p5_y']\n",
    "    model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                              verbose=0)\n",
    "    metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                      categorical_columns, 'drop')\n",
    "    #aggiungi le metriche al dataframe\n",
    "    new_row = {'city': city, 'intersected_cities': \", \".join(map(str, intersected_cities['city'].unique())),\n",
    "               'MAE': metrics['MAE'], 'MSE': metrics['MSE'], 'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "    metrics_df_pca_75_init = pd.concat([metrics_df_pca_75_init, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_pca_75_init"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_pca_75_init.to_csv('../dataset/metrics_df_pca_75_init_range50.csv', index=False)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## tutto e 75 con 3 componenti"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pca_df_3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train a model for each city using nearby cities' data\n",
    "for target_city in pca_df_3['city'].unique():\n",
    "    data_intersected_indexes = calculate_intersected_data(pca_df_3, 2, target_city)\n",
    "\n",
    "    print(f\"Intersected data for {target_city}: {data_intersected_indexes}\")\n",
    "\n",
    "    pca_df_3_ = pca_df_3.copy()\n",
    "    print(pca_df_3_.columns)\n",
    "    # add column: where city is target_city put 1, where index is in data_intersected_indexes put 2, else 0\n",
    "    pca_df_3_['intersection'] = pca_df_3_.apply(\n",
    "        lambda x: 1 if x['city'] == target_city else 2 if x.name in data_intersected_indexes else 0, axis=1)\n",
    "\n",
    "    results[target_city] = pca_df_3_\n",
    "\n",
    "    # scatterplot pca_df_3_ based on intersection class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=pca_df_3_, x='principal_component_1', y='principal_component_2', hue='intersection')\n",
    "    plt.title(f'PCA 2D for {target_city}')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### tutto"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_df_pca_3 = pd.DataFrame(columns=['city', 'intersected_cities', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "for city, data in results.items():\n",
    "    target_city_indexes = data[data['intersection'] == 1].index\n",
    "    intersected_cities_indexes = data[data['intersection'] == 2].index\n",
    "\n",
    "    target_city = df[df.index.isin(target_city_indexes)]\n",
    "    intersected_cities = df[df.index.isin(intersected_cities_indexes)]\n",
    "\n",
    "    print(f\"Testo la città {city}\")\n",
    "    # print unique cities value count\n",
    "    print(target_city['city'].value_counts())\n",
    "    print(intersected_cities['city'].value_counts())\n",
    "\n",
    "    if len(intersected_cities) == 0:\n",
    "        continue\n",
    "\n",
    "    X_train = intersected_cities.drop(columns=['pm2p5_y'])\n",
    "    y_train = intersected_cities['pm2p5_y']\n",
    "    X_test = target_city.drop(columns=['pm2p5_y'])\n",
    "    y_test = target_city['pm2p5_y']\n",
    "    model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                              verbose=0)\n",
    "    metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                      categorical_columns, 'drop')\n",
    "    #aggiungi le metriche al dataframe\n",
    "    new_row = {'city': city, 'intersected_cities': \", \".join(map(str, intersected_cities['city'].unique())),\n",
    "               'MAE': metrics['MAE'], 'MSE': metrics['MSE'], 'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "    metrics_df_pca_3 = pd.concat([metrics_df_pca_3, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_pca_3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_pca_3.to_csv('../dataset/metrics_df_pca_3_intero.csv', index=False)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 75%\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_df_pca_3_75 = pd.DataFrame(columns=['city', 'intersected_cities', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "categorical_columns = ['type_sensor', 'sensor_id', 'day_of_week']\n",
    "\n",
    "for city, data in results.items():\n",
    "    intersected_cities_indexes = data[data['intersection'] == 2].index\n",
    "\n",
    "    target_city_training = pd.read_csv(f'../dataset/training_{city}.csv', parse_dates=['valid_at'])\n",
    "    target_city_testing = pd.read_csv(f'../dataset/testing_{city}.csv', parse_dates=['valid_at'])\n",
    "\n",
    "    intersected_cities = df[df.index.isin(intersected_cities_indexes)]\n",
    "    if len(intersected_cities) == 0:\n",
    "        continue\n",
    "\n",
    "    training = pd.concat([intersected_cities, target_city_training])\n",
    "\n",
    "    print(f\"Testo la città {city}\")\n",
    "    # print unique cities value count\n",
    "    print(training['city'].value_counts())\n",
    "    print(target_city_testing['city'].value_counts())\n",
    "\n",
    "    X_train = training.drop(columns=['pm2p5_y'])\n",
    "    y_train = training['pm2p5_y']\n",
    "    X_test = target_city_testing.drop(columns=['pm2p5_y'])\n",
    "    y_test = target_city_testing['pm2p5_y']\n",
    "    model = lgb.LGBMRegressor(num_leaves=50, learning_rate=0.05, max_depth=-1, n_estimators=500, random_state=982,\n",
    "                              verbose=0)\n",
    "    metrics, model = pipeline_generalizzazione_intero(X_train, y_train, X_test, y_test, model, ['valid_at', 'city'],\n",
    "                                                      categorical_columns, 'drop')\n",
    "    #aggiungi le metriche al dataframe\n",
    "    new_row = {'city': city, 'intersected_cities': \", \".join(map(str, intersected_cities['city'].unique())),\n",
    "               'MAE': metrics['MAE'], 'MSE': metrics['MSE'], 'RMSE': metrics['RMSE'], 'R2': metrics['R2']}\n",
    "    metrics_df_pca_3_75 = pd.concat([metrics_df_pca_3_75, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "metrics_df_pca_3_75\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_df_pca_3_75.to_csv('../dataset/metrics_df_pca_3_75_range50.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### amplified ranges"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cities and their R² values\n",
    "cities_data = {\n",
    "    'Aosta': [0.9046, 0.8926, 0.7981, 0.7668, 0.7609, 0.7242, 0.7140, 0.6827, 0.5262, -0.2862],\n",
    "    'Calgary': [0.9846, 0.9761, 0.9638, 0.6745, -0.1688, -0.7081, 0.6078, -2.2869, -2.0571, -0.8954],\n",
    "    'Badajoz': [0.8903, 0.7905, 0.7067, 0.7550, 0.6827, 0.6522, 0.5752, 0.5238, 0.5323, 0.2876],\n",
    "    'Bangalore': [0.9051, 0.9036, 0.8847, 0.8898, 0.8388, 0.8536, 0.6078, 0.5507, 0.6692, 0.5144],\n",
    "    'Delhi': [0.9023, 0.9042, 0.8993, 0.8280, 0.8223, 0.8273, 0.8145, 0.8062, 0.8045, 0.8029],\n",
    "    'Hamirpur': [0.9598, 0.9590, 0.9408, 0.9441, 0.9378, 0.8956, 0.8972, 0.9070, 0.8907, 0.8828],\n",
    "    'Lima_IQAir': [0.7318, -0.1936, -0.2543, -0.1882, -0.1417, -0.4159, -0.4512, -0.4322, -0.4463, -0.4951],\n",
    "    'Lima_AIRBEAM': [0.8041, 0.8307, 0.8201, 0.7759, 0.7469, 0.7074, 0.7568, -0.2989, 0.0590, 0.2788],\n",
    "    'Southampton_PMS5003': [0.8851, 0.9399, 0.9377, 0.9334, 0.9298, 0.9294, 0.9056, 0.9016, 0.6052, 0.0803],\n",
    "    'Southampton_SPS030': [0.8895, 0.9021, 0.8793, 0.8712, 0.8644, 0.8603, 0.2201, -0.1713, 0.4067, 0.4933]\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_values = list(range(10))\n",
    "\n",
    "for city, r_squared_values in cities_data.items():\n",
    "    plt.plot(x_values, r_squared_values, marker='o', label=city)\n",
    "\n",
    "plt.title('R² Values by Number of Datasets Combined', fontsize=16)\n",
    "plt.xlabel('Number of Datasets Combined', fontsize=12)\n",
    "plt.ylabel('R² Value', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
